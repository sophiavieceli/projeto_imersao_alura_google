{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sophiavieceli/projeto_imersao_alura_google/blob/main/projeto_imersao_alura.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Um chatbot para te ajudar a praticar conversação em qualquer idioma!\n",
        "Este código utiliza o Gemini (a IA generativa do Google) para possibilitar a prática de conversação em um idioma de estudo qualquer e para fazer verificações no seu processo de configuração."
      ],
      "metadata": {
        "id": "-bi96IYdkPEH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZ9QMRwpZGPf"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDzJANz1ZPYN"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('SECRET_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2YXchaaaCVf"
      },
      "outputs": [],
      "source": [
        "model = genai.GenerativeModel(model_name='gemini-1.0-pro')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui, as configurações de regras iniciais da conversa são criadas em funções, para serem chamadas mais adiante na mesma célula. São elas idioma de estudo, nível de fluência do usuário e assunto da conversa."
      ],
      "metadata": {
        "id": "fxpRWSPhmKgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def definir_idioma():\n",
        "  chat = model.start_chat(history=[])\n",
        "\n",
        "  idioma = input('Digite o idioma no qual deseja conversar: \\n')\n",
        "\n",
        "  resposta = chat.send_message(f'{idioma} é um nome de idioma que existe e no qual você é capaz de conversar? Se sim, responda APENAS \"ok\", exatamente assim e nada mais. Se não, respoda APENAS \"erro\", exatamente assim e nada mais.')\n",
        "\n",
        "  while resposta.text == 'erro' or resposta.text == 'Erro':\n",
        "    idioma = input('Digite um idioma válido: \\n')\n",
        "    resposta = chat.send_message(f'{idioma} é um nome de idioma que existe e no qual você é capaz de conversar? Se sim, responda APENAS \"ok\", exatamente assim e nada mais. Se não, respoda APENAS \"erro\", exatamente assim e nada mais.')\n",
        "\n",
        "  return idioma"
      ],
      "metadata": {
        "id": "9clMG5_1SPDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como são poucos os níveis de fluência gerais, definir a sua escolha através de um menu é mais eficiente por não exigir um tempo extra para a validação do Gemini, além de reduzir a possibilidade de erros."
      ],
      "metadata": {
        "id": "g5jZBJCZmdYh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pR7zNIjSX0UB"
      },
      "outputs": [],
      "source": [
        "def definir_nivel_fluencia():\n",
        "  nivel_fluencia = input(f'\\nDigite o número correspondente ao seu nível de fluência em {idioma}:\\n 1 - Básico 1\\n 2 - Básico 2\\n 3 - Intermediário 1\\n 4 - Intermediário 2\\n 5 - Avançado 1\\n 6 - Avançado 2\\n')\n",
        "\n",
        "  while nivel_fluencia != '1' and nivel_fluencia != '2' and nivel_fluencia != '3' and nivel_fluencia != '4' and nivel_fluencia != '5' and nivel_fluencia != '6':\n",
        "    nivel_fluencia = input(f'\\nErro. Digite o número correspondente ao seu nível de fluência em {idioma}:\\n 1 - Básico 1\\n 2 - Básico 2\\n 3 - Intermediário 1\\n 4 - Intermediário 2\\n 5 - Avançado 1\\n 6 - Avançado 2\\n')\n",
        "\n",
        "  if nivel_fluencia == '1':\n",
        "    nivel_fluencia = 'Básico 1'\n",
        "  elif nivel_fluencia == '2':\n",
        "    nivel_fluencia = 'Básico 2'\n",
        "  elif nivel_fluencia == '3':\n",
        "    nivel_fluencia = 'Intermediário 1'\n",
        "  elif nivel_fluencia == '4':\n",
        "    nivel_fluencia = 'Intermediário 2'\n",
        "  elif nivel_fluencia == '5':\n",
        "    nivel_fluencia = 'Avançado 1'\n",
        "  elif nivel_fluencia == '6':\n",
        "    nivel_fluencia = 'Avançado 2'\n",
        "\n",
        "  return nivel_fluencia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usbAq0P83qwS"
      },
      "outputs": [],
      "source": [
        "def definir_assunto():\n",
        "  chat2 = model.start_chat(history=[])\n",
        "\n",
        "  assunto = input(f'\\nInforme o assunto sobre o qual você gostaria de conversar em {idioma}, nível {nivel_fluencia}: \\n')\n",
        "\n",
        "  resposta = chat2.send_message(f'Valide se {assunto} é um assunto não malicioso. Se for válido, responda APENAS com \"ok\", exatamente assim e nada mais. Se não, responda APENAS com \"erro\", exatamente assim e nada mais.')\n",
        "\n",
        "  while resposta.text == 'erro' or resposta.text == 'Erro':\n",
        "    assunto = input(f'O assunto que você escolheu foi considerado inadequado pela IA (considere a possibilidade de erros). Por favor, digite outro assunto: \\n')\n",
        "    resposta = chat2.send_message(f'Valide se {assunto} é um assunto não malicioso. Se for válido, responda APENAS com \"ok\", exatamente assim e nada mais. Se não, responda APENAS com \"erro\", exatamente assim e nada mais.')\n",
        "\n",
        "  return assunto"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como os próximos passos têm uma finalidade diferente para o uso do Gemini, foi criado um novo chat a partir do mesmo modelo, inclusive para usar um histórico zerado."
      ],
      "metadata": {
        "id": "st_6E1DToDip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As instruções de (1) explicar os termos entre hífens e (2) retornar as explicações ao final da conversa poderiam ser passadas no prompt inicial, mas parece que o excesso de informações no mesmo prompt confunde a IA. Por esse motivo, as instruções são passadas no momento necessário.\n",
        "\n"
      ],
      "metadata": {
        "id": "-HgFolcq2O2r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1wGpJL8xP3b",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "chat3 = model.start_chat(history=[])\n",
        "\n",
        "idioma = definir_idioma()\n",
        "nivel_fluencia = definir_nivel_fluencia()\n",
        "assunto = definir_assunto()\n",
        "\n",
        "prompt = f'Gere uma frase imitando uma mensagem de texto comum no idioma {idioma}, no nível de fluência {nivel_fluencia} sobre {assunto}.'\n",
        "'Imagine que você é um falante nativo desse idioma e está me ajudando a praticar a conversação.'\n",
        "\n",
        "print(f'\\nVamos conversar em {idioma}! Digite \"fim\" quando desejar encerrar a conversa.\\n')\n",
        "print(f'Se quiser saber o significado de termos usados pela IA, digite-os divididos por hífen (termo 1 - termo 2) ou, no caso de um único termo, adicione um hífen ao final (termo -)\\n')\n",
        "\n",
        "while prompt != 'fim':\n",
        "  if prompt.count('-') > 0:\n",
        "    resposta = chat3.send_message(f'Explique em português cada uma das palavras listadas em \"{prompt}\"')\n",
        "  else:\n",
        "    resposta = chat3.send_message(prompt)\n",
        "  print('Gemini: ', resposta.text, '\\n')\n",
        "  prompt = input('Você: ')\n",
        "if prompt == 'fim':\n",
        "  prompt = 'Responda com \"Aprendidos durante a nossa conversa: \", e os termos e explicações que você deu durante a conversa.'\n",
        "  resposta = chat3.send_message(prompt)\n",
        "  print('\\nGemini: ', resposta.text, '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Código que permite uma melhor visualização do diálogo, após ele ocorrer:"
      ],
      "metadata": {
        "id": "GLw_5_XdDn-N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mveyZIUiaQKb"
      },
      "outputs": [],
      "source": [
        "import textwrap\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "def to_markdown(text):\n",
        "    text = text.replace('•', '  *')\n",
        "    return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
        "\n",
        "for message in chat3.history:\n",
        "    display(to_markdown(f'**{message.role}**: {message.parts[0].text}'))\n",
        "print('-----------------------------------------------')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyVYwJMFDSiRf0Sjd5Eta+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}